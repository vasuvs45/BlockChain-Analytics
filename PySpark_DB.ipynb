{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement spark-dbscan (from versions: none)\n",
      "ERROR: No matching distribution found for spark-dbscan\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "  pip install spark-dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "import pyarrow.parquet as pq\n",
    "import logging\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#from pyspark.ml.clustering import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data from Parquet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyarrow_table = pq.read_table(\"C:\\\\Users\\\\vasuv\\\\OneDrive\\\\Desktop\\\\DE\\\\AWSBlockChain\\\\datasets\\\\bitcoin\\\\transactions\\\\combined_bitcoin_2024-11-03.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pyarrow_table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hash                       object\n",
       "version                     int64\n",
       "size                        int64\n",
       "block_hash                 object\n",
       "block_number                int64\n",
       "index                       int64\n",
       "virtual_size                int64\n",
       "lock_time                   int64\n",
       "input_count                 int64\n",
       "output_count                int64\n",
       "is_coinbase                  bool\n",
       "output_value              float64\n",
       "outputs                    object\n",
       "block_timestamp    datetime64[ns]\n",
       "date                       object\n",
       "last_modified      datetime64[ns]\n",
       "fee                       float64\n",
       "input_value               float64\n",
       "inputs                     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['block_timestamp','last_modified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName('ReadParquetExample').config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\").config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=C:\\\\Users\\\\vasuv\\\\OneDrive\\\\Desktop\\\\DE\\\\AWSBlockChain\\\\pyspark_pipeline\\\\transactions\\\\log4j.properties\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"4g\")  # Increase driver memory\n",
    "conf.set(\"spark.executor.memory\", \"4g\") # Increase executor memory\n",
    "conf.set(\"spark.driver.maxResultSize\", \"2g\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataCleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df=spark_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_combined_file_path_df=spark_df.select('version','size','block_number','virtual_size','input_count','output_count','is_coinbase','output_value','input_value','fee','date','inputs','outputs')\n",
    "extracted_input_output = spark_df.select('version','size','virtual_size','inputs','outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_combined_file_path_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_input_output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_combined_file_path_df = extracted_combined_file_path_df.withColumn('output_size_ratio',col('output_value')/col('size'))\n",
    "extracted_combined_file_path_df = extracted_combined_file_path_df.withColumn('fee_input_ratio',col('fee') / col('input_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_derived_df = extracted_combined_file_path_df.select('output_size_ratio','fee_input_ratio') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_derived_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataBase Insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration for PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/analytics\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to PostgreSQL\n",
    "extracted_derived_df.write.jdbc(url=jdbc_url, table=\"derived_columns\", mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hash: string, version: bigint, size: bigint, block_hash: string, block_number: bigint, index: bigint, virtual_size: bigint, lock_time: bigint, input_count: bigint, output_count: bigint, is_coinbase: boolean, output_value: double, outputs: array<struct<address:string,index:bigint,required_signatures:bigint,script_asm:string,script_hex:string,type:string,value:double>>, date: string, fee: double, input_value: double, inputs: array<struct<address:string,index:bigint,required_signatures:bigint,script_asm:string,script_hex:string,sequence:bigint,spent_output_index:bigint,spent_transaction_hash:string,txinwitness:array<string>,type:string,value:double>>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df.select([count(when((isnan(c) | col(c).isNull()) if spark_df.schema[c].dataType in [\"DoubleType\", \"FloatType\"] else col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#active_jobs = sc.statusTracker.getActiveJobsIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current SparkContext\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "# # List all active jobs\n",
    "# active_jobs = sc.statusTracker().getActiveJobsIds()\n",
    "# print(\"Active Job IDs:\", active_jobs)\n",
    "\n",
    "# # Kill a specific job (replace job_id with the actual job ID you want to kill)\n",
    "# if active_jobs:\n",
    "#     job_id = active_jobs[0]  # Example: get the first active job\n",
    "#     sc.cancelJob(job_id)\n",
    "# else:\n",
    "#     print(\"No active jobs to cancel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.cancelAllJobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_input_output = spark.read.csv('C:\\\\Users\\\\vasuv\\\\OneDrive\\\\Desktop\\\\DE\\\\AWSBlockChain\\\\extracted_input_output.csv',header=True)\n",
    "# df_derived = spark.read.csv('C:\\\\Users\\\\vasuv\\\\OneDrive\\\\Desktop\\\\DE\\\\AWSBlockChain\\\\extracted_derived_df.csv',header=True)\n",
    "# df_extracted = spark.read.csv('C:\\\\Users\\\\vasuv\\\\OneDrive\\\\Desktop\\\\DE\\\\AWSBlockChain\\\\extracted_combined_file_path_df.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------------+--------------------+-------------+\n",
      "|_c0|version|size|virtual_size|              inputs|      outputs|\n",
      "+---+-------+----+------------+--------------------+-------------+\n",
      "|  1|      2| 430|         298|\"[{\"\"address\"\": \"...| \"\"index\"\": 0|\n",
      "|  2|      1| 225|         144|\"[{\"\"address\"\": \"...| \"\"index\"\": 0|\n",
      "|  3|      1| 350|         189|\"[{\"\"address\"\": \"...| \"\"index\"\": 0|\n",
      "|  4|      2| 222|         141|\"[{\"\"address\"\": \"...| \"\"index\"\": 0|\n",
      "|  5|      1| 223|         141|\"[{\"\"address\"\": \"...| \"\"index\"\": 0|\n",
      "+---+-------+----+------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+--------------------+\n",
      "|_c0|   output_size_ratio|     fee_input_ratio|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|0.000241594023255...|0.008560406274297374|\n",
      "|  2|0.001014843288888889|0.001888345125145...|\n",
      "|  3|2.445714285714285...| 0.04241776091319338|\n",
      "|  4|3.986328828828829...| 0.01766278859378139|\n",
      "|  5|0.000147610089686...|0.004689746508769464|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+------------+-----------+--------------------+-------+-----------------+---------------+\n",
      "|                 _c0|             version|                size|        block_number|     virtual_size|         input_count|        output_count|         is_coinbase|                 fee|       last_modified|      date|    block_timestamp|output_value|input_value|              inputs|outputs|output_size_ratio|fee_input_ratio|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+------------+-----------+--------------------+-------+-----------------+---------------+\n",
      "|                   1|                   2|                 430|              867843|              298|                   2|                   4|               False|          0.00089698|2024-10-29 00:18:...|2024-10-29|2024-10-29 00:17:29|  0.10388543| 0.10478241|[{'address': '37E...|   NULL|             NULL|           NULL|\n",
      "|        '032424b4...|                NULL|                NULL|                NULL|             NULL|                NULL|                NULL|                NULL|                NULL|                NULL|      NULL|               NULL|        NULL|       NULL|                NULL|   NULL|             NULL|           NULL|\n",
      "|       dtype=object)| 'type': 'scripth...| 'value': 0.10477...|                NULL|             NULL|                NULL|                NULL|                NULL|                NULL|                NULL|      NULL|               NULL|        NULL|       NULL|                NULL|   NULL|             NULL|           NULL|\n",
      "| {'address': 'bc1...|          'index': 1| 'required_signat...|    'script_asm': ''| 'script_hex': ''| 'sequence': 4294...| 'spent_output_in...| 'spent_transacti...| 'txinwitness': a...|                NULL|      NULL|               NULL|        NULL|       NULL|                NULL|   NULL|             NULL|           NULL|\n",
      "|       dtype=object)| 'type': 'witness...| 'value': 5.46e-0...|[{'address': 'bc1...|             NULL|                NULL|                NULL|                NULL|                NULL|                NULL|      NULL|               NULL|        NULL|       NULL|                NULL|   NULL|             NULL|           NULL|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+------------+-----------+--------------------+-------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_input_output.show(5)\n",
    "# df_derived.show(5)\n",
    "# df_extracted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- virtual_size: string (nullable = true)\n",
      " |-- inputs: string (nullable = true)\n",
      " |-- outputs: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- output_size_ratio: string (nullable = true)\n",
      " |-- fee_input_ratio: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- block_number: string (nullable = true)\n",
      " |-- virtual_size: string (nullable = true)\n",
      " |-- input_count: string (nullable = true)\n",
      " |-- output_count: string (nullable = true)\n",
      " |-- is_coinbase: string (nullable = true)\n",
      " |-- fee: string (nullable = true)\n",
      " |-- last_modified: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- block_timestamp: string (nullable = true)\n",
      " |-- output_value: string (nullable = true)\n",
      " |-- input_value: string (nullable = true)\n",
      " |-- inputs: string (nullable = true)\n",
      " |-- outputs: string (nullable = true)\n",
      " |-- output_size_ratio: string (nullable = true)\n",
      " |-- fee_input_ratio: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_input_output.printSchema()\n",
    "# df_derived.printSchema()\n",
    "# df_extracted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_input_output=df_input_output.withColumn('version',df_input_output['version'].cast(IntegerType()))\\\n",
    "#                                 .withColumn('size',df_input_output['size'].cast(IntegerType()))\\\n",
    "#                                 .withColumn('virtual_size',df_input_output['virtual_size'].cast(IntegerType()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|version|sum(size)|\n",
      "+-------+---------+\n",
      "|      1| 57371639|\n",
      "|      2|161561942|\n",
      "|      3|      205|\n",
      "+-------+---------+\n",
      "\n",
      "+-------+---------+\n",
      "|version|min(size)|\n",
      "+-------+---------+\n",
      "|      1|      150|\n",
      "|      2|      150|\n",
      "|      3|      205|\n",
      "+-------+---------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|version|avg(virtual_size)|\n",
      "+-------+-----------------+\n",
      "|      1|408.4527873786179|\n",
      "|      2|227.1695456705289|\n",
      "|      3|            154.0|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|version|max(virtual_size)|\n",
      "+-------+-----------------+\n",
      "|      1|            97408|\n",
      "|      2|            99895|\n",
      "|      3|              154|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|version|      stddev(size)|\n",
      "+-------+------------------+\n",
      "|      1|3494.4863060843104|\n",
      "|      2|3220.1745461839178|\n",
      "|      3|              NULL|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_input_output.groupBy('version').sum('size').show()\n",
    "# df_input_output.groupBy('version').min('size').show()\n",
    "# df_input_output.groupBy('version').mean(\"virtual_size\").show()\n",
    "# df_input_output.groupBy('version').max(\"virtual_size\").show()\n",
    "# df_input_output.groupBy('version').agg({'size':'stddev'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|version|  collect_list(size)|\n",
      "+-------+--------------------+\n",
      "|      1|[225, 350, 223, 2...|\n",
      "|      2|[430, 222, 371, 1...|\n",
      "|      3|               [205]|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|version|   collect_set(size)|\n",
      "+-------+--------------------+\n",
      "|      1|[843, 356, 2207, ...|\n",
      "|      2|[3702, 53727, 139...|\n",
      "|      3|               [205]|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_input_output.groupby('version').agg(collect_list('size')).show()\n",
    "# df_input_output.groupby('version').agg(collect_set('size')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_counts = df_input_output.select([\n",
    "#     (F.count(F.when(F.col(c).isNull() | F.col(c).isNaN(), c))).alias(c) for c in df_input_output.columns\n",
    "# ])\n",
    "\n",
    "# combined_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data from database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Minimal connection test\n",
    "# try:\n",
    "#     df = spark.read.jdbc(\n",
    "#         #url=\"jdbc:postgresql://localhost:5432/analytics\",\n",
    "#         url = \"jdbc:postgresql://localhost:5432/analytics?user=postgresql&password=postgresql\",\n",
    "#         table=\"input_output\",\n",
    "#         properties={\n",
    "#             \"user\": \"postgresql\",\n",
    "#             \"password\": \"postgresql\",\n",
    "#             \"driver\": \"org.postgresql.Driver\"\n",
    "#         }\n",
    "#     )\n",
    "#     print(\"Connection successful, driver loaded.\")\n",
    "#     df.show(1)  # Display the first row to confirm data load\n",
    "# except Exception as e:\n",
    "#     print(\"Connection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", url) \\\n",
    "#     .option(\"dbtable\", \"input_output\") \\\n",
    "#     .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "#     .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(spark.conf.get(\"spark.jars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"jdbc:postgresql://localhost:5432/analytics?user=postgresql&password=postgresql\"\n",
    "#url = \"jdbc:postgresql://localhost:5432/postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"PySparkApp\").config(\"spark.jars\", \"C:\\\\spark\\\\jars\\\\postgresql-42.7.4.jar\").getOrCreate()\n",
    "#spark = SparkSession.builder.appName(\"PySparkApp\").config(\"spark.jars\", \"C:\\\\spark\\\\jars\\\\postgresql-42.7.4.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"jdbc:postgresql://<your_host>:<your_port>/<your_database>\"\n",
    "#url = \"jdbc:postgresql://postgres:postgres@localhost:5432/analytics\"\n",
    "# url = \"jdbc:postgresql://localhost:5432/analytics\"\n",
    "# properties={\n",
    "#     \"user\":\"postgresql\",\n",
    "#     \"password\":\"postgresql\",\n",
    "#     \"driver\":\"org.postgresql.Driver\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_input_output_db = spark.read.jdbc(url=url,table=\"input_output\",properties=properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
