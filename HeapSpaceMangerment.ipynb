{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, increase the Java heap space by setting these configurations before creating your SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"4g\")  # Increase driver memory\n",
    "conf.set(\"spark.executor.memory\", \"4g\") # Increase executor memory\n",
    "conf.set(\"spark.driver.maxResultSize\", \"2g\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If that's not enough, you can try writing in batches instead of all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_in_batches(df, jdbc_url, table_name, batch_size=10000):\n",
    "    total_rows = df.count()\n",
    "    num_partitions = (total_rows + batch_size - 1) // batch_size\n",
    "    \n",
    "    df.repartition(num_partitions).write \\\n",
    "        .option(\"batchsize\", batch_size) \\\n",
    "        .jdbc(url=jdbc_url,\n",
    "              table=table_name,\n",
    "              mode=\"append\",\n",
    "              properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another approach is to repartition your DataFrame before writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition to smaller chunks\n",
    "extracted_derived_df = extracted_derived_df.repartition(10)  # adjust number based on your data size\n",
    "\n",
    "# Then write\n",
    "extracted_derived_df.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"derived_columns\",\n",
    "    mode=\"append\",\n",
    "    properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also try setting specific JDBC batch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.update({\n",
    "    \"batchsize\": \"1000\",  # Adjust based on your needs\n",
    "    \"isolationLevel\": \"READ_COMMITTED\"\n",
    "})\n",
    "\n",
    "extracted_derived_df.write \\\n",
    "    .option(\"numPartitions\", 10) \\\n",
    "    .jdbc(url=jdbc_url,\n",
    "          table=\"derived_columns\",\n",
    "          mode=\"append\",\n",
    "          properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-memory 8g --executor-memory 8g pyspark-shell'\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"8g\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"100\")  # Increase shuffle partitions\n",
    "conf.set(\"spark.default.parallelism\", \"100\")     # Increase parallelism\n",
    "conf.set(\"spark.memory.fraction\", \"0.8\")         # Give more memory to execution\n",
    "conf.set(\"spark.memory.storageFraction\", \"0.2\")  # Reduce storage fraction\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Now, let's break down the data into much smaller chunks\n",
    "df_size = extracted_derived_df.count()\n",
    "target_size = 500  # KB per partition\n",
    "num_partitions = max(df_size // 1000, (19674 // target_size) + 1)  # Calculate based on current task size\n",
    "\n",
    "# Repartition and write in smaller chunks\n",
    "(extracted_derived_df\n",
    " .repartition(num_partitions)\n",
    " .write\n",
    " .option(\"batchsize\", 100)  # Smaller batch size\n",
    " .option(\"numPartitions\", num_partitions)\n",
    " .jdbc(url=jdbc_url,\n",
    "       table=\"derived_columns\",\n",
    "       mode=\"append\",\n",
    "       properties={**properties,\n",
    "                  \"rewriteBatchedStatements\": \"true\",\n",
    "                  \"batchsize\": \"100\",\n",
    "                  \"isolationLevel\": \"READ_COMMITTED\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas and write in chunks\n",
    "pandas_df = extracted_derived_df.toPandas()\n",
    "chunk_size = 1000  # Adjust based on your memory constraints\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from itertools import islice\n",
    "\n",
    "def write_chunk(chunk_df):\n",
    "    # Convert chunk back to spark dataframe\n",
    "    spark_chunk = spark.createDataFrame(chunk_df.to_dict('records'))\n",
    "    spark_chunk.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"derived_columns\",\n",
    "        mode=\"append\",\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "# Process in chunks\n",
    "for i in range(0, len(pandas_df), chunk_size):\n",
    "    chunk = pandas_df.iloc[i:i + chunk_size]\n",
    "    write_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas and write in chunks\n",
    "pandas_df = extracted_derived_df.toPandas()\n",
    "chunk_size = 1000  # Adjust based on your memory constraints\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from itertools import islice\n",
    "\n",
    "def write_chunk(chunk_df):\n",
    "    # Convert chunk back to spark dataframe\n",
    "    spark_chunk = spark.createDataFrame(chunk_df.to_dict('records'))\n",
    "    spark_chunk.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"derived_columns\",\n",
    "        mode=\"append\",\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "# Process in chunks\n",
    "for i in range(0, len(pandas_df), chunk_size):\n",
    "    chunk = pandas_df.iloc[i:i + chunk_size]\n",
    "    write_chunk(chunk)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
