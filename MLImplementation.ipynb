{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "import pyarrow.parquet as pq\n",
    "import logging\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyarrow_table = pq.read_table(\"C:\\\\Users\\\\vasuv\\\\OneDrive\\\\Desktop\\\\DE\\\\AWSBlockChain\\\\datasets\\\\bitcoin\\\\transactions\\\\combined_bitcoin_2024-11-03.parquet\")\n",
    "raw_df=pyarrow_table.to_pandas()\n",
    "pandas_df = raw_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['input_count', 'output_count', 'input_value', 'output_value', 'size']\n",
    "X = pandas_df[features]\n",
    "y = pandas_df['fee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fee Prediction using following Algorithms and visualizing them in PowerBI\n",
    "\n",
    "1. Linear Regression\n",
    "2. Random Forest Regressor\n",
    "3. Gradient Boosting\n",
    "4. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 2.3059934196089068e-24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize and train the model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "print(\"Linear Regression MSE:\", mse_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor MSE: 3.685178625204256e-09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize and train the model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(\"Random Forest Regressor MSE:\", mse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting MSE: 3.807682043777177e-09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize and train the model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting MSE:\", mse_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vasuv\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\vasuv\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\vasuv\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\vasuv\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "14228/14228 [==============================] - 104s 7ms/step - loss: 1.8559e-05\n",
      "Epoch 2/10\n",
      "14228/14228 [==============================] - 83s 6ms/step - loss: 1.7703e-05\n",
      "Epoch 3/10\n",
      "14228/14228 [==============================] - 44s 3ms/step - loss: 1.7556e-05\n",
      "Epoch 4/10\n",
      "14228/14228 [==============================] - 44s 3ms/step - loss: 1.7557e-05\n",
      "Epoch 5/10\n",
      "14228/14228 [==============================] - 84s 6ms/step - loss: 1.7468e-05\n",
      "Epoch 6/10\n",
      "14228/14228 [==============================] - 59s 4ms/step - loss: 1.7431e-05\n",
      "Epoch 7/10\n",
      "14228/14228 [==============================] - 33s 2ms/step - loss: 1.7471e-05\n",
      "Epoch 8/10\n",
      "14228/14228 [==============================] - 37s 3ms/step - loss: 1.7368e-05\n",
      "Epoch 9/10\n",
      "14228/14228 [==============================] - 36s 3ms/step - loss: 1.7362e-05\n",
      "Epoch 10/10\n",
      "14228/14228 [==============================] - 36s 3ms/step - loss: 1.7353e-05\n",
      "14228/14228 [==============================] - 25s 2ms/step\n",
      "LSTM MSE: 1.6925783282405315e-05\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Scale data for better LSTM performance\n",
    "# scaler = MinMaxScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# y_scaled = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# # Reshape input data\n",
    "# X_train_reshaped = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# # Define LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "# model.add(Dense(1))\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train_reshaped, y_scaled, epochs=3, batch_size=32, verbose=1)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred_lstm = model.predict(X_train_reshaped)\n",
    "# mse_lstm = mean_squared_error(y_scaled, y_pred_lstm)\n",
    "# print(\"LSTM MSE:\", mse_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming y_test is the actual fees and y_pred_* are predictions from each model\n",
    "results_lr = pd.DataFrame({'actual_fee': y_test, 'predicted_fee': y_pred_lr, 'algorithm': 'Linear Regression'})\n",
    "results_rf = pd.DataFrame({'actual_fee': y_test, 'predicted_fee': y_pred_rf, 'algorithm': 'Random Forest'})\n",
    "results_gb = pd.DataFrame({'actual_fee': y_test, 'predicted_fee': y_pred_gb, 'algorithm': 'Gradient Boosting'})\n",
    "#results_lstm = pd.DataFrame({'actual_fee': y_test, 'predicted_fee': y_pred_lstm.flatten(), 'algorithm': 'LSTM'})\n",
    "\n",
    "# Concatenate all results\n",
    "all_results = pd.concat([results_lr, results_rf, results_gb])\n",
    "all_results['error'] = all_results['actual_fee'] - all_results['predicted_fee']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, JSON\n",
    "engine = create_engine('postgresql://postgres:postgres@localhost:5432/analytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results.to_sql('fee_prediction_results', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analmoly Detection using ML algorithms:\n",
    "1. Isolation Forest\n",
    "2. DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.05, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.05, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(contamination=0.05, random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and train Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)  # Adjust contamination as needed\n",
    "iso_forest.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasuv\\AppData\\Local\\Temp\\ipykernel_23380\\2246266683.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pandas_df['anomaly_label'] = iso_forest.predict(X)\n",
      "C:\\Users\\vasuv\\AppData\\Local\\Temp\\ipykernel_23380\\2246266683.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pandas_df['anomaly_label'] = pandas_df['anomaly_label'].apply(lambda x: 1 if x == -1 else 0)\n",
      "C:\\Users\\vasuv\\AppData\\Local\\Temp\\ipykernel_23380\\2246266683.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pandas_df['algorithm'] = 'Isolation Forest'\n"
     ]
    }
   ],
   "source": [
    "# Predict anomalies (-1 for anomalies, 1 for normal)\n",
    "pandas_df['anomaly_label'] = iso_forest.predict(X)\n",
    "pandas_df['anomaly_label'] = pandas_df['anomaly_label'].apply(lambda x: 1 if x == -1 else 0)\n",
    "pandas_df['algorithm'] = 'Isolation Forest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the results in a new table (e.g., 'transaction_anomalies')\n",
    "pandas_df[['hash', 'input_count', 'output_count', 'input_value', 'output_value', 'fee', 'size', 'anomaly_label', 'algorithm']].to_sql('transaction_anomalies', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>version</th>\n",
       "      <th>size</th>\n",
       "      <th>block_hash</th>\n",
       "      <th>block_number</th>\n",
       "      <th>index</th>\n",
       "      <th>virtual_size</th>\n",
       "      <th>lock_time</th>\n",
       "      <th>input_count</th>\n",
       "      <th>output_count</th>\n",
       "      <th>is_coinbase</th>\n",
       "      <th>output_value</th>\n",
       "      <th>outputs</th>\n",
       "      <th>block_timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>fee</th>\n",
       "      <th>input_value</th>\n",
       "      <th>inputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28860</th>\n",
       "      <td>cceb93583857b4888d547d5b3111c40c2238ab9066d9cb...</td>\n",
       "      <td>2</td>\n",
       "      <td>194</td>\n",
       "      <td>00000000000000000000100fb6ab8bd9e3c405ac8dd4fb...</td>\n",
       "      <td>868714</td>\n",
       "      <td>2818</td>\n",
       "      <td>113</td>\n",
       "      <td>868712</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>[{'address': '1K4sbvUia3ADUJBtHTYEJSSQsTvudk5E...</td>\n",
       "      <td>2024-11-03 17:08:51</td>\n",
       "      <td>2024-11-03</td>\n",
       "      <td>2024-11-03 17:09:25.950594</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>[{'address': 'bc1qz500yhs2u6xsllrsqyvweh4ymxvp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    hash  version  size  \\\n",
       "28860  cceb93583857b4888d547d5b3111c40c2238ab9066d9cb...        2   194   \n",
       "\n",
       "                                              block_hash  block_number  index  \\\n",
       "28860  00000000000000000000100fb6ab8bd9e3c405ac8dd4fb...        868714   2818   \n",
       "\n",
       "       virtual_size  lock_time  input_count  output_count  is_coinbase  \\\n",
       "28860           113     868712            1             1        False   \n",
       "\n",
       "       output_value                                            outputs  \\\n",
       "28860        0.0004  [{'address': '1K4sbvUia3ADUJBtHTYEJSSQsTvudk5E...   \n",
       "\n",
       "          block_timestamp        date              last_modified       fee  \\\n",
       "28860 2024-11-03 17:08:51  2024-11-03 2024-11-03 17:09:25.950594  0.000002   \n",
       "\n",
       "       input_value                                             inputs  \n",
       "28860     0.000403  [{'address': 'bc1qz500yhs2u6xsllrsqyvweh4ymxvp...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "import pyarrow.parquet as pq\n",
    "import logging\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyarrow_table = pq.read_table(\"C:\\\\Users\\\\vasuv\\\\OneDrive\\\\Desktop\\\\DE\\\\AWSBlockChain\\\\datasets\\\\bitcoin\\\\transactions\\\\combined_bitcoin_2024-11-03.parquet\")\n",
    "df = pyarrow_table.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['block_timestamp','last_modified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"4g\")  # Increase driver memory\n",
    "conf.set(\"spark.executor.memory\", \"4g\") # Increase executor memory\n",
    "conf.set(\"spark.driver.maxResultSize\", \"2g\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df=spark_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_combined_file_path_df=spark_df.select('version','size','block_number','virtual_size','input_count','output_count','is_coinbase','output_value','input_value','fee','date','inputs','outputs')\n",
    "extracted_input_output = spark_df.select('version','size','virtual_size','inputs','outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('output_size_ratio',col('output_value')/col('size'))\n",
    "spark_df = spark_df.withColumn('fee_input_ratio',col('fee') / col('input_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['input_count', 'output_count', 'input_value', 'output_value', 'fee', 'size', 'virtual_size']\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = assembler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----+--------------------+------------+-----+------------+---------+-----------+------------+-----------+------------+--------------------+----------+-------+-----------+--------------------+--------------------+\n",
      "|                hash|version|size|          block_hash|block_number|index|virtual_size|lock_time|input_count|output_count|is_coinbase|output_value|             outputs|      date|    fee|input_value|              inputs|            features|\n",
      "+--------------------+-------+----+--------------------+------------+-----+------------+---------+-----------+------------+-----------+------------+--------------------+----------+-------+-----------+--------------------+--------------------+\n",
      "|a71b2c011d2e5ffcb...|      1| 222|00000000000000000...|      868676| 1963|         141|        0|          1|           2|      false|  0.00463238|[{bc1q9nskxamcset...|2024-11-03|8.46E-6| 0.00464084|[{bc1qrg0vtepuw7a...|[1.0,2.0,0.004640...|\n",
      "+--------------------+-------+----+--------------------+------------+-----+------------+---------+-----------+------------+-----------+------------+--------------------+----------+-------+-----------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Scale the features\n",
    "# Standardize the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(df)\n",
    "df = scaler_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DBSCAN with chosen hyperparameters\n",
    "# Initialize DBSCAN with chosen hyperparameters\n",
    "# Initialize DBSCAN with chosen hyperparameters\n",
    "columns = ['input_count', 'output_count', 'input_value', 'output_value', 'size']\n",
    "sampled_df = pandas_df.sample(frac=0.1, random_state=42)\n",
    "sampled_data = pd.DataFrame(sampled_df.values.tolist())\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust `eps` and `min_samples` based on your data\n",
    "sampled_data['cluster'] = dbscan.fit_predict(X)\n",
    "\n",
    "# Label outliers\n",
    "sampled_data['anomaly_label'] = sampled_data['cluster'].apply(lambda x: 1 if x == -1 else 0)\n",
    "sampled_data['algorithm'] = 'DBSCAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up database connection\n",
    "engine = create_engine('your_database_connection_string')  # Replace with your actual DB connection string\n",
    "\n",
    "# Store the DataFrame with anomaly labels in a new table (e.g., 'transaction_anomalies_dbscan')\n",
    "pandas_df[['hash', 'input_count', 'output_count', 'input_value', 'output_value', 'fee', 'size', 'cluster', 'anomaly_label', 'algorithm']].to_sql('transaction_anomalies_dbscan', con=engine, if_exists='replace', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
